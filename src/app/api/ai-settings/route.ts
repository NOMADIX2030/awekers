import { NextRequest, NextResponse } from "next/server";
import prisma from "../../../lib/prisma";
import { CacheManager } from '../../../lib/admin/CacheManager';
import { QueryOptimizer } from '../../../lib/admin/QueryOptimizer';

// ğŸ¯ ìºì‹œ í‚¤ ìƒìˆ˜ ì •ì˜ (AI Settings)
const CACHE_KEYS = {
  AI_SETTINGS: 'ai:settings:all'
};

// ğŸš€ ê·¹ëŒ€í™”ëœ ìºì‹œ TTL
const CACHE_TTL = 1800; // 30ë¶„ (AI ì„¤ì •ì€ ìì£¼ ë³€ê²½ë˜ì§€ ì•ŠìŒ)

// GET: AI ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° (Cache-First ìµœì í™”)
export async function GET() {
  const startTime = performance.now();
  const cache = CacheManager.getInstance();
  
  try {
    // ğŸš€ Phase 1: ì „ì²´ ìºì‹œ í™•ì¸
    const cachedResult = await cache.get(CACHE_KEYS.AI_SETTINGS);
    if (cachedResult) {
      const responseTime = (performance.now() - startTime).toFixed(2);
      console.log(`ğŸš€ AI ì„¤ì • ì „ì²´ ìºì‹œ íˆíŠ¸: ${responseTime}ms`);
      
      return NextResponse.json({
        ...cachedResult,
        cached: true,
        responseTime: `${responseTime}ms`,
        cacheHit: 'FULL'
      });
    }

    // ğŸ¯ Phase 2: DB ì¿¼ë¦¬ ì‹¤í–‰
    console.log('ğŸ’¾ AI ì„¤ì • DB ì¡°íšŒ í•„ìš”');
    const settings = await prisma.siteSetting.findMany({
      where: {
        key: {
          in: ['ai_model', 'ai_max_tokens', 'ai_style', 'ai_system_prompt']
        }
      }
    });

    const defaultSettings = {
      model: 'gpt-3.5-turbo',
      maxTokens: 6000,
      style: 'íŠ¸ë Œë””',
      systemPrompt: 'ë„ˆëŠ” ìµœì‹  íŠ¸ë Œë“œì— ë°ì€ í•œêµ­ì–´ ë¸”ë¡œê·¸ ì‘ê°€ì•¼. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì£¼ì œì— ëŒ€í•´ ìƒì„¸í•˜ê³  ìœ ìµí•œ ë¸”ë¡œê·¸ ê¸€ì„ ìƒì„±í•´.'
    };

    const result = { ...defaultSettings };
    
    settings.forEach(setting => {
      switch (setting.key) {
        case 'ai_model':
          result.model = setting.value;
          break;
        case 'ai_max_tokens':
          result.maxTokens = parseInt(setting.value);
          break;
        case 'ai_style':
          result.style = setting.value;
          break;
        case 'ai_system_prompt':
          result.systemPrompt = setting.value;
          break;
      }
    });

    // ğŸš€ ì „ì²´ ê²°ê³¼ ìºì‹œ ì €ì¥
    await cache.set(CACHE_KEYS.AI_SETTINGS, result, CACHE_TTL);

    const responseTime = (performance.now() - startTime).toFixed(2);
    console.log(`ğŸ¯ AI ì„¤ì • ë¡œë”© ì™„ë£Œ: ${responseTime}ms (${settings.length}ê°œ ì„¤ì •)`);

    return NextResponse.json({
      success: true,
      settings: result,
      responseTime: `${responseTime}ms`
    });

  } catch (error) {
    console.error("âŒ AI ì„¤ì • API ì˜¤ë¥˜:", error);
    return NextResponse.json(
      { success: false, error: 'AI ì„¤ì • ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.' },
      { status: 500 }
    );
  }
}

// POST: AI ì„¤ì • ì €ì¥ (ì¤‘ì•™ì§‘ì¤‘ì‹)
export async function POST(req: NextRequest) {
  const startTime = performance.now();
  
  try {
    const { model, maxTokens, style, systemPrompt } = await req.json();

    // ğŸš€ ì¤‘ì•™ì§‘ì¤‘ì‹ ìµœì í™” ì ìš© (QueryOptimizer íŒ¨í„´)
    console.log('ğŸ¯ AI ì„¤ì • ì €ì¥: QueryOptimizer ì ìš© ì‹œì‘');

    // ğŸš€ ì¤‘ì•™ì§‘ì¤‘ì‹: ëª¨ë“  ì„¤ì • ì €ì¥ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
    await QueryOptimizer.getInstance().executeParallel({
      model: () => prisma.siteSetting.upsert({
        where: { key: 'ai_model' },
        update: { value: model },
        create: { key: 'ai_model', value: model }
      }),
      maxTokens: () => prisma.siteSetting.upsert({
        where: { key: 'ai_max_tokens' },
        update: { value: maxTokens.toString() },
        create: { key: 'ai_max_tokens', value: maxTokens.toString() }
      }),
      style: () => prisma.siteSetting.upsert({
        where: { key: 'ai_style' },
        update: { value: style },
        create: { key: 'ai_style', value: style }
      }),
      systemPrompt: () => prisma.siteSetting.upsert({
        where: { key: 'ai_system_prompt' },
        update: { value: systemPrompt },
        create: { key: 'ai_system_prompt', value: systemPrompt }
      })
    });

    console.log('âœ… AI ì„¤ì • ì €ì¥: QueryOptimizer ìµœì í™” ì™„ë£Œ');
    const endTime = performance.now();
    console.log(`ğŸ¯ AI ì„¤ì • ì €ì¥ ì™„ë£Œ: ${(endTime - startTime).toFixed(2)}ms (4ê°œ ì„¤ì •)`);

    return NextResponse.json({ 
      success: true,
      message: "ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
      responseTime: `${(endTime - startTime).toFixed(2)}ms`
    });
  } catch (error) {
    const endTime = performance.now();
    console.error(`âš ï¸ AI ì„¤ì • ì €ì¥ ì‹¤íŒ¨: ${(endTime - startTime).toFixed(2)}ms`, error);
    return NextResponse.json({ error: "ì„¤ì • ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤." }, { status: 500 });
  }
} 